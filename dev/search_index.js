var documenterSearchIndex = {"docs":
[{"location":"001.fgsm/#FGSM-for-Adversarial-Attacks","page":"FGSM","title":"FGSM for Adversarial Attacks","text":"","category":"section"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"FGSM is a simple algorithm that ...","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"first we can import the Adversarial Module and access the MNIST data from Flux","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"using Adversarial\nusing Flux\nusing Flux: update!\nusing Flux.Data.MNIST\nusing Statistics\n# using Flux.Tracker: gradient, update!\n# using CuArrays # comment out if a GPU is not available\nusing Plots","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"next we will load the training. For the example, we will only perform adversarial attacks on this data","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"train_images = MNIST.images();\ntrain_labels = MNIST.labels();","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"A function to conver the images to arrays","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"function minibatch(i, batch_size = 32)\n    x_batch = Array{Float32}(undef, size(train_images[1])..., 1, batch_size)\n    y_batch = Flux.onehotbatch(train_labels[i:i+batch_size-1], 0:9)\n    for (idx, image) in enumerate(train_images[i:i+batch_size-1])\n        x_batch[:,:,1,idx] = Float32.(image)\n    end\n    return x_batch |> gpu, y_batch |> gpu\nend","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"minibatch (generic function with 2 methods)","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"We then create and train a simple CNN.","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"CNN() = Chain(\n    Conv((3, 3), 1=>16, pad=(1,1), relu),\n    MaxPool((2,2)),\n    Conv((3, 3), 16=>32, pad=(1,1), relu),\n    MaxPool((2,2)),\n    Conv((3, 3), 32=>32, pad=(1,1), relu),\n    MaxPool((2,2)),\n    x -> reshape(x, :, size(x, 4)),\n    Dense(288, 10),\n    softmax) |> gpu\n\nm = CNN();\nθ = params(m);\nloss(x, y) = Flux.crossentropy(m(x), y)\nacc(x, y) = mean(Flux.onecold(m(x)) |> cpu .== Flux.onecold(y) |> cpu)\nopt = ADAM()","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"ADAM(0.001, (0.9, 0.999), IdDict{Any,Any}())","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"now for our training loop...","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"const EPOCHS = 5\nconst BATCH_SIZE = 32\n\nfor epoch in 1:EPOCHS\n    losses = 0.\n    accuracies = 0.\n    steps = 0\n\n    for i in 1:BATCH_SIZE:size(train_images,1)-BATCH_SIZE\n        local l;\n        x, y = minibatch(i)\n        gs = gradient(θ) do\n            l = loss(x, y)\n            return l\n        end\n        a = acc(x, y)\n\n        update!(opt, θ, gs)\n\n        losses += (l |> cpu)\n        accuracies += (a |> cpu)\n        steps += 1\n    end\n    @info \"Epoch $epoch end. Loss: $(losses / steps), Acc: $(accuracies / steps)\"\nend","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"Now that we have a \"trained\" model, lets create some adversarial examples using the FGSM method.","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"Let's begin with a single image","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"x, y = minibatch(1, 1)\nx_adv = FGSM(m, loss, x, y; ϵ = 0.07)\n\n# we can see that the predicted labels are different\nadversarial_pred = m(x_adv) |> Flux.onecold |> getindex\noriginal_pred = m(x) |> Flux.onecold |> getindex","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"6","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"and visualise the resulting adversarial in comparison to the original image. When using an ϵ value of 0.07, the different is very slight, if noticable at all.","category":"page"},{"location":"001.fgsm/","page":"FGSM","title":"FGSM","text":"l = @layout [a b]\nadv = heatmap(permutedims(x_adv, (4, 3, 1, 2))[1,1,:,:] |> cpu)\norg = heatmap(permutedims(x, (4, 3, 1, 2))[1,1,:,:] |> cpu)\nplot(org, adv, layout = l)\n\n@assert adversarial_pred != original_pred","category":"page"},{"location":"blackbox/#BlackBox-Adversarial-Algorithms","page":"BlackBox","title":"BlackBox Adversarial Algorithms","text":"","category":"section"},{"location":"blackbox/","page":"BlackBox","title":"BlackBox","text":"SimBA","category":"page"},{"location":"whitebox/#WhiteBox-Adversarial-Algorithms","page":"WhiteBox","title":"WhiteBox Adversarial Algorithms","text":"","category":"section"},{"location":"whitebox/","page":"WhiteBox","title":"WhiteBox","text":"FGSM\nPGD\nJSMA\nCW\nDeepFool","category":"page"},{"location":"whitebox/#Adversarial.FGSM","page":"WhiteBox","title":"Adversarial.FGSM","text":"FGSM(model, loss, x, y; ϵ = 0.1, clamp_range = (0, 1))\n\nFast Gradient Sign Method (FGSM) is a method of creating adversarial examples by pushing the input in the direction of the gradient and bounded by the ε parameter.\n\nThis method was proposed by Goodfellow et al. 2014 (https://arxiv.org/abs/1412.6572)\n\nArguments:\n\nmodel: The model to base the attack upon.\nloss: The loss function to use. This assumes that the loss function includes   the predict function, i.e. loss(x, y) = crossentropy(model(x), y).\nx: The input to be perturbed by the FGSM algorithm.\ny: The 'true' label of the input.\nϵ: The amount of perturbation to apply.\nclamp_range: Tuple consisting of the lower and upper values to clamp the input.\n\n\n\n\n\n","category":"function"},{"location":"whitebox/#Adversarial.PGD","page":"WhiteBox","title":"Adversarial.PGD","text":"PGD(model, loss, x, y; ϵ = 10, step_size = 0.1, iters = 100, clamp_range = (0, 1))\n\nProjected Gradient Descent (PGD) is an itrative variant of FGSM with a random point. For every step the FGSM algorithm moves the input in the direction of the gradient bounded in the l∞ norm. (https://arxiv.org/pdf/1706.06083.pdf)\n\nArguments:\n\nmodel: The model to base teh attack upon.\nloss: the loss function to use, assuming that it includes the prediction function   i.e. loss(x, y) = crossentropy(m(x), y)\nx: The input to be perturbed.\ny: the ground truth for x.\nϵ: The bound around x.\nstep_size: The ϵ value in the FGSM step.\niters: The maximum number of iterations to run the algorithm for.\nclamp_range: The lower and upper values to clamp the input to.\n\n\n\n\n\n","category":"function"},{"location":"whitebox/#Adversarial.JSMA","page":"WhiteBox","title":"Adversarial.JSMA","text":"JSMA(model, x, t; Υ, θ)\n\nJacobian Saliency Map Algorithm (JSMA), craft adversarial examples by modifying a very small amount of pixels. These pixels are selected via the jacobian matrix of the output w.r.t. the input of the network. (https://arxiv.org/pdf/1511.07528.pdf)\n\nArguments:\n\nmodel: The model to create adversarial examples for.\nx: The original input data\nt: Index corrosponding to the target class (this is a targeted attack).\nΥ: The maximum amount of distortion\nθ: The amount by which each feature is perturbed.\n\n\n\n\n\n","category":"function"},{"location":"whitebox/#Adversarial.CW","page":"WhiteBox","title":"Adversarial.CW","text":"CW(model, x, t; dist = euclidean, c = 0.1)\n\nCarlini & Wagner's (CW) method for generating adversarials through the optimisation of a loss function against a target class. Here we consider the F6 variant loss function. (https://arxiv.org/pdf/1608.04644.pdf)\n\nArguments:\n\nmodel: The model to attack.\nx: The original input data\nt: Index label corrosponding to the target class.\ndist: The distance measure to use L0, L2, L∞. Assumes this is from the   Distances.jl library or some other callable function.\nc: value for the contribution of the missclassification in the error function.\n\n\n\n\n\n","category":"function"},{"location":"whitebox/#Adversarial.DeepFool","page":"WhiteBox","title":"Adversarial.DeepFool","text":"DeepFool(model, x, overshoot = 0.02, max_iter = 50)\n\nMoosavi-Dezfooli et al.'s (https://arxiv.org/pdf/1511.04599.pdf) DeepFool method.\n\nAn algorithm to determine the minimum perturbation needed to change the class assignment of the image. This algorithm is useful then for computing a robustness metric of classifiers, where as other algorithms (such as FGSM) may return sub-optimal solutions for generating an adversarial.\n\nThe algorithm operates in a greedy way, such that, its not guaranteed to converge to the smallest possible perturbation (that results in an adversarial). Despite this shortcoming, it can often yield a class approximation.\n\nThe python/matlab implementations mentioned in the paper can be found at: https://github.com/LTS4/DeepFool/\n\nArguments:\n\nmodel: The flux model to attack before the softmax function.\nimage: An array of input images to create adversarial examples for. (size, WHC)\novershoot: The halting criteria to prevent vanishing gradient.\nmax_iter: The maximum iterations for the algorithm.\n\n\n\n\n\n","category":"function"},{"location":"#Adversarial.jl-Documentation","page":"Home","title":"Adversarial.jl Documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Adversarial attacks for Neural Networks written with FluxML.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Adversarial examples are inputs to Neural Networks (NNs) that result in a miss-classification. Through the exploitation that NNs are susceptible to slight changes (perturbations) of the input space, adversarial examples are often indistinguishable from the original input from the point of view of a human.","category":"page"},{"location":"","page":"Home","title":"Home","text":"A common example of this phenomenon is from the use of the Fast Gradient Sign Method (FGSM) proposed by Goodfellow et al. 2014 https://arxiv.org/abs/1412.6572 where the gradient information of the network can be used to move the pixels of the image in the direction of gradient, and thereby increasing the loss for the resulting image. Despite the very small shift in pixels, this is enough for the NN to miss-classify the image: https://pytorch.org/tutorials/beginner/fgsm_tutorial.html","category":"page"},{"location":"","page":"Home","title":"Home","text":"We have included some of the common methods to create adversarial examples, this includes:","category":"page"},{"location":"","page":"Home","title":"Home","text":"Fast Gradient Sign Method (FGSM)\nProjected Gradient Descent (PGD)\nJacobian-based Saliency Map Attack (JSMA)\nCarlini & Wagner (CW)","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"You can install this package through Julia's package manager in the REPL:","category":"page"},{"location":"","page":"Home","title":"Home","text":"] add Adversarial","category":"page"},{"location":"","page":"Home","title":"Home","text":"or via a script:","category":"page"},{"location":"","page":"Home","title":"Home","text":"using Pkg; Pkg.add(\"Adversarial\")","category":"page"},{"location":"#Quick-Start-Guide","page":"Home","title":"Quick Start Guide","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"As an example, we can create an adversarial image using the FGSM method:","category":"page"},{"location":"","page":"Home","title":"Home","text":"x_adv = FGSM(model, loss, x, y; ϵ = 0.07)","category":"page"},{"location":"","page":"Home","title":"Home","text":"Where model is the FluxML model, loss is some loss function that uses a predict function, for example crossentropy(model(x), y). x is the original input, y is the true class label, and \\epsilon is a parameter that determines how much each pixel is changed by.","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
